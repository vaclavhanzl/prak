#! /usr/bin/env python3
intro = """
prak_prongen.py - generate possible Czech pronunciations from text transcript

Copyright (c) 2022 Vaclav Hanzl. This is a free software (see the MIT license).

This file is part of the https://github.com/vaclavhanzl/prak project but
can be also used standalone. You can move it around and/or modify the included
pronunciation tables to let it suite your needs. To learn more, try:

   prak_prongen.py --help
"""

lexicon_txt = """
### Add your rules here:



### Longer match wins, use "+written+ pronounced" form to get the absolute priority.
### Sequential order of rules does not matter.
### (Only if exactly the same string is replaced by two rules, the later rule prevails.)
### Word boundary is denoted by '+' (this character counts in match length).

### Built-in rules:
+tkanič tkanič kanič
citronád citronád
citron citrón
multi multy
+js s js
+ODS+ ódées
panick panick panyck
proud prOd pro=ud
medi medy
mechani mechany
+anti +anty
tika+ tyka+
### štika paštika
štika+ štika+
gramati gramaty
### použitý
použ po=už
### poukázal vypouklé poukázky
pouk pouk po=uk
vypoukl vypoukl
poukázk poukázk
poukázc poukázc
### poučně hloupoučké tupoučké poučka
pouč po=uč
upoučk upoučk
### pouliční poulil   (po=ulil zanedbám)
pouličn po=uličn
### nepousmál
pousm po=usm
### prouhelněný černouhelný biouhel kamenouhelný
ouhel o=uhel

rakouskouher rakousko=uher

### rychloupínací
oupína o=upína

prouzen pro=uzen

### poupravit
poupr po=upr

videoukázk video=ukázk
audioukázk audio=ukázk
videoukázc video=ukázc
audioukázc audio=ukázc

### pouvažuj
pouv po=uv

### mimounijní mimouniverzitní pantomimou
+mimou mimo=u

### nízkouhlíkový sirouhlík okrouhlík radiouhlíková
okrouhlík orkouhlík
ouhlík o=uhlík

### doučte pseudoučence z Radouče
radouč radouč
douč do=uč

### celounijní 
+celoun celo=un
+celouk celo=uk
+celous celo=us

### doužíval pseudouživatel
+douží do=uží
+pseudou pseudo=u

### vnitrounijní
ounij o=unij

samouk samo=uk
samouč samo=uč

trojúhe troj=úhe

+třía tří=a
+třío tří=o
+tříú tří=ú
+příu pří=u
+vyu vy=u
+výu vý=u

### This rather exceptional rule maps to 'E', an internal symbol for the 'eu' diphthong.
+eur Er


### additional hand-made rules for ditini (most cases are guessed well by autogenerated rules)
titul tytul
tinitus tynytus
stati+ stati
+nivel nyvel
+tik tik
+tike tyke
biti+ biti
ziti+ ziti
žiti+ žiti
niti+ niti
diti+ diti
anditi+ andyti
granit+ granyt
uni+ uni

nitrotol nytrotol
nitrolak nytrolak

+dirk dirk

nikotin nykotyn

nihil nyhil


+exa egza
+exe egze
+exi egzi
+exo egzo
+exu egzu
+exhi egzhi
+exha egzha
+neexi neegzi


### Some Czech peculiarities
+dcer cer
+srdc src

+nesh ne=sh
+nash na=sh

takhle takhle takle tagle

osm+ osm osum
osum osum osm
sedm+ sedm sedum
sedum sedm sedum

osmd osmd osumd osnd
sedmd sedmd sedumd sednd

milion milion milión
balkon balkon balkón
stadion stadyon stadyón

### very foreign words
+hyje+ hyjé
+messenger+ mesendžr mesindžr
+game+ gejm
+room+ rům
### can be English or French
+nation+ nejšn nasion
washington vošingtn
t-mobile týmobajl
siemens símens
interview intervjů
+the+ d
###+usa+ úesá ú=es=á <-- not needed like this
+usa+ ú=es=á
windows vindous


### TODO: CIA in capitals should work here:
+cia+ síajej
+e-mail ýmejl
+kč+ káčé korunčeských
+ods+ ó=dé=es
+eu+ é=ú
+dph+ dé=pé=há
+pc+ pé=cé
+cm+ centymetr centymetrů
+čsl+ čéesel

"""

# These rules are normally mixed with those above. Use long enough substring to override if needed.
autogen_lexicon_replacements_txt = """
### Autogenerated rules for ditini/dytyny
+di+ +dy+\n+dil +dyl\n+dim +dym\n+din +dyn\n+dis +dys\n+edi +edy\n+eti +ety\n+etni +etny
+hosti +hosti\n+ini +iny\n+lati +lati\n+mani +many\n+ni +ni\n+nikoli +nikoli\n+niky +nyky\n+nil +nyl
+nin +nyn\n+tib +tyb\n+tim +tym\n+tip +typ\n+uni +uny\nadiu adyu\naiti aity\nakti akty\nalti alty
ameni ameni\nanic anic anyc\nanim anym\naniz anyz\nardi ardy\natib atyb\natici atici\natie atye
atif atyf\natih atyh\natik atyk\natino atyno\nativ atyv\natiz atyz\natič atyč\naudi audy\nauti auty
bani bany\nbsti bsty\nbáni bány\ncidi cidy\ncti cti\ncyni cyny\ncéni cény\ndia dya\ndici dyci
dick dyck\ndida dyda\ndido dydo\ndidá dydá\ndie dye\ndif dyf\ndig dyg\ndih dyh\ndii dyi\ndij dyj
dik dyk\ndile dyle\ndime dyme\ndimo dymo\nding dyng\ndio dyo\ndip dyp\ndir dyr\ndis+ dys+\ndisc dysc
disi dysi\ndisp dysp\ndita dyta\nditn dytn\ndito dyto\nditu dytu\ndity dyty\ndiu+ dyu+\ndiver dyver
divi dyvi\ndiz dyz\ndiá dyá\ndií dyí\ndiš+ dyš+\ndni dni\ndnic dnic\ndoni dony\ndrti drti\ndti dty
eati eaty\nechni echny\nedic edyc\nedim edym\nedič edyč\neni eni\nenick enyck\nenis enys\nerdi erdy
erni erny\nesni esni\nestin estyn\netic etyc\netie etye\netik etyk\netič etyč\neuti euty\nfini finy
fiti fity\nfoni fony\ngani gany\ngati gaty\ngeni geny\ngeti gety\ngiti gity\ngni gny\ngoni gony
goti goty\ngusti gusty\nhani hany\nhati hati haty\nhatic hatic\nhiti hity\nhnik hnyk\nhnič hnyč
hti hti\nháni hány\niati iaty\nidiš idyš\nieni ieny\nikti ikty\ninia inya\ninih ynyh\ninik inyk
inim inym\ninis inys\ninti inty\niodi iody\nioti ioty\niti+ ity+\nitic ityc\nitik ityk\nitim itym
itin itin\nitiz ityz\njidi jidy\njuni juny\nkani kany\nketi kety\nklini kliny\nkni kni\nktic ktyc
ktid ktyd\nktik ktyk\nldi ldy\nlnic lnic\nlotil lotyl\nltif ltyf\nltik ltyk\nltim ltym\nltip ltyp
ltiv ltyv\nlyti lyty\nléni lény\nmedi medy\nmeti mety\nmini miny\nmni mni\nmodi mody\nmoni mony
mrti mrti\nmuni muny\nmíti míty\nmýti mýty\nnati naty\nndi ndy\nnedi nedy\nneti nety\nni ni ny
ni+ ni+\nnia+ nya+\nnial nyal\nniat nyat\nniba nyba\nnic nic\nnick nick nyck\nnicky nycky\nnicr nycr
nidů nydů\nnie nye\nnien nyen\nnif nyf\nnig nyg\nnigé nygé\nnii nyi\nnijn nyjn\nnik nik\nnika+ nyka+
nikaj nikaj\nnikar nykar\nnikl+ nikl+\nnikno nikno\nniko niko nyko\nnikoh nikoh\nnikola nykola
nikoly nykoly\nnikom nikom\nnikou nykou\nniká+ niká+\nnil nil\nnilu nylu\nnimo nymo\nnimu nymu
nimá nymá\nnin nin\nnink nynk\nnint nynt\nnio nyo\nnip nyp\nnir nyr\nnirv nyrv\nnis+ nys+\nnism nysm
niss nyss\nnist nyst\nnisu nysu\nnit nit\nnita nyta\nniti nyty\nnitn nytn\nnitu nytu\nnity nyty
niu nyu\nnivk nivk\nniza nyza\nnizm nyzm\nnizu nyzu\nnizá nyzá\nniá nyá\nniáln nyáln\nnií nyí
nič nič\nničt nyčt\nniš niš\nniž niž\nnoni nony\nnrati nrati\nnsti nsty\nntib ntyb\nntic ntyc
ntid ntyd\nntif ntyf\nntik ntyk\nntil ntyl\nntim ntym\nntin ntyn\nntis ntys\nntit ntyt\nntiv ntyv
odni odni\noeti oety\nohni ohni\nonia onya\nonick onyck\nonis onys\noniz onyz\nonti onty\nordi ordy
orti orty\nostiž ostiž\notani otany\notivo otivo\notni otni\nparti party\npati paty\npedi pedy
peti pety\npoctiv poctiv\npostiže postiže\nproti proti\npti pty\nrani rani\nrdis rdys\nredi redy
reni reny\nresti resty\nreti rety\nrini riny\nristi risty\nriti rity\nrkti rkty\nrnic rnic
rnis rnys\nrniz rnyz\nroni rony\nrti rti rty\nrti+ rti+\nrtic rtyc\nrtie rtye\nrtif rtyf\nrtik rtyk
rtim rtym\nrtin rtin\nréni rény\nsadis sadys\nsanti santy\nseni seny\nsidi sidy\nsni sni\nsnic snic
soni sony\nstani stani\nsti+ sti+\nstie stye\nstik styk\nstip styp\nstir styr\nstiž stiž styž
stni stni\nsvíti svíti\ntadi tady\ntati taty\nteni teny\nteti tety\ntia tya\ntibi tybi\ntibu tybu
tic tyc\ntich tich\ntidy tydy\ntidě tydě\ntie+ tye+\ntifi tyfi\ntifu tyfu\ntig tyg\ntiho+ tyho+
tii tyi\ntii+ tyi+\ntik+ tyk+\ntika tyka\ntike tyke\ntiku tyku\ntiky tyky\ntiká tyká\ntiků tyků
tiln tyln\ntilu tylu\ntilá tylá\ntim+ tym+\ntima tyma\ntime tyme\ntimn tymn\ntimo tymo\ntimu tymu
timá tymá\ntin tin tyn\ntin+ tin+\ntina tina\nting tyng\ntini tyny\ntiniho tynyho\ntinu+ tinu+
tiny tiny\ntině tině\ntio tyo\ntipe type\ntipl typl\ntipů typů\ntirl tyrl\ntis+ tys+\ntise tyse
tism tysm\ntisíc tisíc\ntisů tysů\ntita tyta\ntitu tytu\ntity tyty\ntitě tytě\ntiv tyv\ntiv+ tyv+
tiva tyva\ntivc tyvc\ntive tyve\ntivi tyvi\ntivo tivo tyvo\ntivu tyvu\ntivy tyvy\ntiza tyza
tizo tyzo\ntizu tyzu\ntiá tyá\ntií tyí\ntičn tyčn\ntičt tyčt\ntnik tnyk\ntodi tody\ntoni tony
tradi trady\ntti tty\ntuni tuny\ntádi tády\ntáni tány\nudium udium udyum\nukti ukty\numani umany
uni uny\nuničo uničo\nupni upni\nustin ustyn\nuti uti\nvlasti vlasti\nvni vni\nvnic vnic\nvnit vnit
vrti vrti\nxti xty\nydi ydy\nzni zni\násti ásti\nédi édy\níti íti\nódi ódy\nčeti četi\nčini čini
čisti čisti\nčnic čnic\nčti čti\něni ěni\něti ěti\nřeti řeti\nřnic řnic\nšesti šesti\nšti šti
žnic žnic
"""


# seam inventory:
#
# ' ' ... can become '|' or '_'
# '|'
# '_'
# '~' ... used by glue_prepos()
############ those above separate words, those below DO NOT
# '='    použ po=už


# '-'  not used as seam (?)
# '+'

# What touches seams:

#  glue_prepos() glues by '~'
#  deleted interpunction:   '"'+".,?!„“-=–:;—/_"

#  align_wav_and_text_using_model() ???????



import argparse
import sys
import os
from glob import glob
import unicodedata


#print("Remove inport here")
#from hmm_pron import HMM


def transform(table, text):
    "Apply each replacement in table"
    for f, t in table:
        text = text.replace(f, t)
    return text

def line_iterable_to_table(iterable):
    """
    Create replacement table from iterable providing lines
    """
    table = []
    for line in iterable:
        if line.startswith('###'):
            continue  # ignore comment lines
        elems = line.split() # also ignores final newline
        len_elems = len(elems)
        if len_elems == 0:
            continue  # ignore empty lines
        elif len_elems == 1:
            f, t = elems[0], "" # delete rule
        elif len_elems == 2:
            f, t = elems # substitute rule
        else:
            print(f'Wrong line in the substitution table: "{line}"', file=sys.stderr)
            sys.exit()
            
        table.append((f, t))
    return table

def string_to_table(string):
    """
    Convert replacement table in string to realtime representation
    """
    return line_iterable_to_table(string.splitlines())

def read_replacement_table(filename):
    "Read replacement table from file of lines like 'from to'"
    table = []
    with open(filename, encoding='utf-8') as file: # explicit utf-8 needed on Windows
        for line in file:
            if line.startswith('###'):
                continue  # ignore comment lines
            elems = line.split() # also ignores final newline
            len_elems = len(elems)
            if len_elems == 0:
                continue  # ignore empty lines
            elif len_elems == 1:
                f, t = elems[0], "" # delete rule
            elif len_elems == 2:
                f, t = elems # substitute rule
            else:
                print(f'Wrong line in the substitution table: "{line}"', file=sys.stderr)
                sys.exit()
            
            table.append((f, t))
    return table


interpunction_to_delete = '"'+".,?!„“-=–:;—/_|~"  # Maybe '_|' should rather map to '=' (seam) ?
# IDEA: We might rather map ANY interpunction to seam and then remove seams next to spaces

downcase = ("AÁBCČDĎEĚÉFGHIÍJKLĹĽMNŇOÓÔPQRŔŘSŠTŤUÚŮVWXYÝZŽÄÜÖ",
            "aábcčdďeěéfghiíjklĺľmnňoóôpqrŕřsštťuúůvwxyýzžäüö")

voiceassim_inert = "rjlmnň"
vowels_and_diphtongs = "AEOaeiouyáéíóúůýě"

voiceassim_context_splitters = vowels_and_diphtongs + voiceassim_inert

voiced_voiceless_pairs = "hH GH vf zs dt žš bp řŘ gk ďť Zc Žč"

"MN@?+"

voiced = ""; voiceless = ""
voice = {}; devoice = {};
for vcd, vls in voiced_voiceless_pairs.split():
    voiced += vcd; voiceless += vls
    voice[vls] = vcd; devoice[vcd] = vls
# Fix minor irregularities:
voice['H'] = 'G' # e.g. voicing 'ch' in 'abych byl'

for p in voiceassim_context_splitters + " ":
    voice[p] = p; devoice[p] = p



"""
mv Mv
mf Mf
"""


def clean_textline(line):
    """
    Ensure NFC, no BOM and no CR/LF at the end of line
    """
    if line and line[0] == '\uFEFF':
        line = line[1:]
    line = line.rstrip("\r\n")
    line = unicodedata.normalize('NFC', line)
    return line


def make_text_unix_clean_and_NFC(text):
    """
    Ensure Unix newline conventions, NFC and no BOMs
    """
    out_text = ""
    for line in text.splitlines(): # usual CRLFs are gone already here
        out_text += clean_textline(line) + "\n" # still, some wild CRLF combinations may be trimmed here
    return out_text





def line_iterable_to_lexirules(iterable):
    """
    Create lexicon rules from iterable providing lines
    """
    lexirules = {}
    for line in iterable:
        line = clean_textline(line) # Ensure NFC, no BOM and no CR/LF
        if line=="":
            continue
        if line.startswith('###'):
            continue  # ignore comment lines
        line = line.split()
        if len(line)==0: # e.g. just "\n" when reading from exceptions file
            continue
        if len(line)==1: # very dangerous, would produce empty sets in sausages
            print(f'Prak WARNING: Ignoring rule for "{line[0]}" with ZERO replacements.', file=sys.stderr)
            continue
        pattern, *replacements = line
        lexirules[pattern] = replacements
    return lexirules

def read_lexirules_table(filename):
    """
    Read lexicon rules from a text file
    """
    with open(filename, encoding='utf-8') as file: # explicit utf-8 needed on Windows
        return line_iterable_to_lexirules(file)


spelltab = string_to_table("""
### Czech spelling. This cannot be done well by the main pronunciation rules
### as dots are already deleted (and letter downcased) by the time we gwet there.
### So thete is a special early pass just for spelling using this table.
### This table is incomplete, e.g. Á or W are missing.
A. á
B. bé
C. cé
D. dé
E. é
F. ef
G. gé
H. há
I. í
J. jé
K. ká
L. el
M. em
N. en
O. ó
P. pé
Q. kvé
R. er
S. es
T. té
U. ú
V. vé
X. yks
Y. ypsilon
Z. zet
""")


phonetizetab = string_to_table("""
### Basic replacements to get approx. phone-per-grapheme
ch H
tz c
###ll l
### looks like ss,rr can be done here, unlike ll
ss s
rr r
dž Ž
x ks
ů ú
q kv
w v
dž Ž
dz Z
ou O
au A
###eu E
ü u
ö o
è e
ä a
ï y
""")







interpunctab = [(ch,'') for ch in interpunction_to_delete]

downcasetab = list(zip(*downcase))

to_cz_transcription = string_to_table("""
### Convert our phone set to "Czech" transcription as published in
### the book by Volín & Skarnitzl, Segmentální plán češtiny, page 102.
### Added symbols: ř̥ d͡z ʔ ɣ ɱ ŋ
ý i:
y ı
é e:
e e
á a:
a a
ó o:
o o
ú u:
u u
O o͡u
A a͡u
E e͡u
p p
b b
t t
d d
ť ť
ď ď
k k
g g
f f
v v
s s
z z
š š
ž ž
H x
h h
G ɣ
c c
č č
Ž d͡ž
Z d͡z
ř ř
Ř ř̥
r r
m m
M ɱ
n n
N ŋ
ň ň
j j
l l
? ʔ
### o⁀u   @ +
""")


rr_forward_assim = string_to_table("""
### Forward loss of voiced quality (exceptional forward rule,
### all the other similar things work backward)
tř tŘ
kř kŘ
Hř HŘ
př pŘ
fř fŘ
sř sŘ
šř šŘ
cř cŘ
čř čŘ
### "W" is either "h" or "H"
+sh +sW
=sh =sW
""")


phone_merging = line_iterable_to_lexirules("""
###
tš tš č
ts ts c
ye+ yje
kk kk k
ll ll l
### rozsáhlá
ss ss s
zs ss s
šš šš š
žš šš š
rr r
nň ň
nn n
zz zz z
### some sure cases are already converted to E
eu E eu
###+eur Er
""".split("\n"))


def all_substrings(txt):
    """
    Return set of all possible substrings of txt
    """
    retval = set()
    for subs_len in range(len(txt),0,-1):
        for pos in range(0, len(txt)-subs_len+1):
            retval.add(txt[pos:pos+subs_len])
    return retval

# FINISHME  do ?   Anebo ani od ?
prepos = set("od bez nad pod před ob z v s u k".split())

def glue_prepos(text):
    """
    Glue some prepositions to word where needed for
    voicing assimilations.
    TODO: Needs multiple outputs for "bez"
    """
    out = ""
    for word in text.split():
        out += word
        if word in prepos:
            #out += "_"
            #out += "="     # bez uzardění - needs to know there is a seam
            out += '~'      # need char different from marker used in composed words
        else:
            out += ' '
    return out.strip()

# Word separation: '_' .. glued together, '|' .. separated by a (short) pause, ' ' .. not
# yet decided which of the previous two types this separation will be
# NOTE: Use '‖' (\u2016) for presentation output (instead of "|")?


# All these assimilators work backwards

# bitmap:

softening = 1 # sending a hook to the left
softening_by_hooked_e = 2 # there was ě
voicing = 4 # serious voicing attack to the left
devoicing = 8 # serious devoicing attach to the left
maystop = 16 # glottal stop possible if the last vowel/diphtong happens to be word initial
velarize = 32 # k,g influencing preceding n

soften = {'d':"ď", "t":"ť", "n":"ň"}
harden = {'i':"y", "í":"ý", "ě":"e"}

def expand_pluriphones(phone, context):
    """
    Expand "W" into two possibilities: "h" and "H".
    Used in at tricky treatment o exceptional and optional
    forward assimilation in "sh".
    """
    if phone=="W":
        return [("h", context), ("H", context)]
    return [(phone, context)]

def dtn_assim(inp, context): # context is int bitmap
    """
    Convert input phone and context to list of couples
    (output_phone, context), solving possible assimilations
    like dň -> ďň.
    TODO: on seam?   odnikud odnimatelný      process =
    """
    if inp in "bpmvf" and context&softening_by_hooked_e:
        if inp=="m":
            inserted_phone = "ň"
        else:
            inserted_phone = "j"
        return [(inp+inserted_phone, context&~softening&~softening_by_hooked_e)]
    context &= ~softening_by_hooked_e # for all branches below
    
    if inp in "ďťň":
        return [(inp, context|softening), (inp, context&~softening)]
    elif inp in "dtn" and context&softening:
        return [(soften[inp], context), (soften[inp], context&~softening)]
    elif inp in "iíě":
        if inp=="ě":
            context |= softening_by_hooked_e
        return [(harden[inp], context|softening)]
    else:
        return [(inp, context&~softening)]


def glottal_stop_insert(phone, context):
    """
    Insert optional glottal stop before initial vowels
    """
    if phone in "&": # special 'phone' to get out all latent possibilities
        retval = [("", context&~maystop)]
        if context&maystop:
            retval += [("?", context&~maystop)]
        return retval
    if phone in "_" and context&maystop:
        context &= ~maystop
        return [("_", context), ("_?", context)]
    if phone in "|" and context&maystop:
        context &= ~maystop
        return [("|?", context)]
    if phone in " " and context&maystop:
        context &= ~maystop
        return [("_", context), (" ?", context)]

    if phone in "=~" and context&maystop: # seam ('~' variant is from glue_prepos())
        context &= ~maystop
        return [(phone, context), (phone+"?", context)]

    if phone in vowels_and_diphtongs:
        if phone in "iíyý" and context&maystop:
            phone += "j"
        context |= maystop
    else:
        context &= ~maystop
    return [(phone, context)]


def voicing_assim(phone, context):
    """
    Do voiced/voiceles assimilations
    """
    # prepare possible return values
    context_n = context
    context_n &= ~voicing
    context_n &= ~devoicing # neither flag set (neutral, but can do ANYTHING on word boundary)
    context_v = context_n|voicing # voicing flag set
    context_d = context_n|devoicing # devoicing flag set

    # 3 possible contexts on a word boundary: voicing, devoicing, none of these
    # One answer is always devoicing+pause.
    # If no pause:
    #         voicing or devoicing move on to act on the word on the left
    #         otherwise two options:
    #                  act like voicing to the left word
    #                  act like DEVOICING to the left (!! really: "mroš_mrznul")

    if phone in '=~': # composed word seam, just pass on the context
        return [(phone, context)]

    if phone == "*": # special 'phone', disappears but leaves both voicing possiblities
        return [('', context_d), ('', context_v)]
    
    if phone == '_': # already decided no-pause word boundary
        if context&voicing or context&devoicing: # just pass on these decided cases
            return [('_', context)]
        return [('_', context_d), ('_', context_v)] # otherwise allow both

    if phone == ' ': # word boundary with optional pause        
        answers = [('|', context_d)] # one answer is always devoicing+pause
        if not context&voicing:
            answers.append(('_', context_d)) # devoicing, no pause
        if not context&devoicing:
            answers.append(('_', context_v)) # voicing, no pause
        # maybe both were appended
        return answers

    # Intra-word. Our phone is possibly assimilated via the incoming context:
    if context&voicing and phone in voice:
        phone = voice[phone]
    elif context&devoicing and phone in devoice:
        phone = devoice[phone]

    # Now decide what context we pass on:
    if phone in voiced and not phone=='v':
        context = context_v 
    elif phone in voiceless or phone=='?':
        context = context_d
    else:
        context = context_n # neutral intra-word but quite a wild card on a word boundary

    return [(phone, context)]


def velarize_nkg(phone, context):
    """
    k,g influencing preceding n
    """
    if phone=="n" and context&velarize:
        phone = "N"
    if phone in "kg":
        context |= velarize
    else:
        context &= ~velarize
    return [(phone, context)]


### End of Czech rules. Technical realisation of a composed system follows. ###

def fun_multiphone(fun, multiphone, context):
    """
    Like fun but accepts several phones, not just
    exactly one.
    """
    outputs_and_contexts = {("", context)}
    for phone in multiphone:
        new_outputs_and_contexts = set() # set of variant partial prons, possibly bigger each step
        for out, con in outputs_and_contexts:
            for new_phones, new_con in fun(phone, con):
                new_outputs_and_contexts.add((out+new_phones[::-1], new_con)) # reverting new_phones
        outputs_and_contexts = new_outputs_and_contexts
    return outputs_and_contexts

def all_assim(phone, context):
    """
    Do all assimilations, chained
    """
    phones_and_contexts = [(phone, context)]

    # can do the 2 loops in both orders?
    for fun in [expand_pluriphones, glottal_stop_insert, voicing_assim, dtn_assim, velarize_nkg]:
    #for fun in [glottal_stop_insert]: # dtn_assim should be last, can produce 2 phones
       
        new_phones_and_contexts = []
        for phone, con in phones_and_contexts:
            #for new_phone, new_con in fun(phone, con):
            for new_phone, new_con in fun_multiphone(fun, phone, con):
                new_phones_and_contexts.append((new_phone, new_con))
        phones_and_contexts = new_phones_and_contexts

    return phones_and_contexts


def common_substrings(set_of_txts):
    """
    Return set of substrings found in all txts
    """
    retval = None
    for txt in set_of_txts:
       substrings = all_substrings(txt)
       #print(substrings)
       if retval == None:
           retval = substrings
       else:
           retval &= substrings
    return retval


def longest_common_substring(set_of_txts):
    """
    Return longest substring present in all txts.
    If there are multiple, return the lexicographically
    first one (to make it deterministic).
    """
    substrings = common_substrings(set_of_txts)
    if not substrings:
        return ""
    max_len = max(len(x) for x in substrings)
    max_substrings = [s for s in substrings if len(s)==max_len]
    max_substrings.sort()
    return max_substrings[0]


def split_before_after_substring(txt, substring):
    """
    txt must contain substring. Return couple with
    parts of txt before and after the substring.
    """
    i = txt.index(substring)
    return txt[:i], txt[i+len(substring):]


def cartesian_plus(txtlist1, txtlist2):
    """
    Return list with all combinations of texts from the first
    list and from the second.
    Both arguments and also the return value are lists of strings.
    """
    return [t1+t2 for t1 in txtlist1 for t2 in txtlist2]


def try_factorize_beg_mid_end(set_of_txts):
    """
    Try to factorize sausage section into three sections with different
    begins, common middle and different ends. Return either the 3-section
    list if successfull or a one section list with the original set.
    This is a "reasonable effort" heuristic which should often work
    in situations where it is used here but may fail to find optimization
    even if it exists.
    The begin and/or end part can be empty but the mid part cannot.
    Even if this procedure gives uncertain results, it takes substantial
    amount of CPU time. Sure there are better ways... Feel free to find them.
    """
    mid = longest_common_substring(set_of_txts)

    if mid=="":
        # cowardly give up
        return [set_of_txts]
    beg_set = set()
    end_set = set()
    for txt in set_of_txts:
        beg, end = split_before_after_substring(txt, mid)
        beg_set.add(beg)
        end_set.add(end)

    # Now do the final test - does our factorization generate
    # exactly the input set of texts?
    cp = set(cartesian_plus(cartesian_plus(beg_set, {mid}), end_set))
    if cp!=set_of_txts:
        # after much more effort, give up anyway
        return [set_of_txts]
    # Factorization worked! Great! Open a bottle of Champagne!
    retval = []
    if beg_set!={""}:
        retval.append(beg_set)
    retval.append({mid})
    if end_set!={""}:
        retval.append(end_set)
    return retval


def try_factorize_cut46(set_of_txts):
    """
    Try to factorize sets of aternatives into 2*2, 2*3 etc.
    combinations by trying all cut points if txts are all
    of the same length and short.
    (Such sets often arise as a leftover after bme factorization
    or arise by themselves.)
    """
    if len(set_of_txts) not in {4, 6, 8, 9, 10, 12, 14, 15, 16}: #small non-primes
        return [set_of_txts] #give up, factorization impossible or costly
    len_set = {len(txt) for txt in set_of_txts}
    len_min = min(len_set) # take the smallest member
    len_max = max(len_set) # take the smallest member

    cut_limit = len_min
    if len_max>len_min:
        cut_limit += 1 # allow also an empty string in the right set

    for cut_point in range(1, cut_limit):
        beg_set = set()
        end_set = set()
        for txt in set_of_txts:
            beg = txt[:cut_point]
            end = txt[cut_point:]
            beg_set.add(beg)
            end_set.add(end)

        # Now do the final test - does our factorization generate
        # exactly the input set of texts?
        cp = set(cartesian_plus(beg_set, end_set))
        if cp!=set_of_txts:
            continue
        else:
            return [beg_set, end_set]

    return [set_of_txts] #no cut point worked
    

def bme_factorize_sausages(ssgs):
    """
    Try to split each section with multiple variants into multiple
    sections with less variants where such factorization succeeds.
    This is a costly, yet heuristic and unsure optimization.
    """
    while True:
        ssgs_tmp = []
        for s in ssgs:
            ssgs_tmp += try_factorize_beg_mid_end(s)
        ssgs_tmp_2 = []
        for s in ssgs_tmp:
            ssgs_tmp_2 += try_factorize_cut46(s)
        ssgs_tmp_2 = reversed_sausages(ssgs_tmp_2) # needed for "v USA je to tak"
        ssgs_out = []
        for s in ssgs_tmp_2:
            ssgs_out += try_factorize_cut46(s)
        ssgs_out = reversed_sausages(ssgs_out)
        if ssgs_out==ssgs: # did we converge?
            break
        ssgs = ssgs_out # no, more loops needed
    return ssgs_out


def ssgs_process_phones(ssgs):
    ssgs = reversed_sausages(ssgs) # we do everything backward, as most assimilations work backward
    outputs_and_contexts = {("", devoicing)} # init: just 1 variant, final devoicing
    out_ssgs = []
    for section in ssgs:
        section_outputs_and_contexts = set() # each input variant may result in several variants here
        single_variant = len(section)==1 # we can do more agressive joining for single variant
        for variant in section:
            variant_outputs_and_contexts = outputs_and_contexts
            for phone in variant:
                new_outputs_and_contexts = set() # set of variant partial prons, possibly bigger each step
                for out, con in variant_outputs_and_contexts:
                    for new_phone, new_con in all_assim(phone, con):
                        new_outputs_and_contexts.add((out+new_phone, new_con))
                variant_outputs_and_contexts = new_outputs_and_contexts

                if single_variant:

                    if (len({context for text, context in variant_outputs_and_contexts}) == 1 or
                        len({text for text, context in variant_outputs_and_contexts}) == 1):
                        alternatives = {text for text, context in variant_outputs_and_contexts}
                        variant_outputs_and_contexts = {
                            ("", context) for text, context in variant_outputs_and_contexts}
                        if alternatives!={""}: # TEST NOT NEEDED?
                            out_ssgs.append(alternatives)

            # Now one variant of the input sausage section was fully processed
            section_outputs_and_contexts |= variant_outputs_and_contexts
        outputs_and_contexts = section_outputs_and_contexts

        if (len({context for text, context in outputs_and_contexts}) == 1 or
            len({text for text, context in outputs_and_contexts}) == 1):
            alternatives = {text for text, context in outputs_and_contexts}
            outputs_and_contexts = {("", context) for text, context in outputs_and_contexts}
            if alternatives!={""}: # This test is needed (why? when are empty strings produced?)
                out_ssgs.append(alternatives)

    alternatives = {text for text, context in outputs_and_contexts}
    if alternatives!={""}:
        out_ssgs.append(alternatives)
    return reversed_sausages(out_ssgs) # we worked on it backword, return it in natural order



def reintroduce_space(sausages):
    """
    Where alternatives differ just by the endings being "|" and "_",
    replace these two by a single one with " ".
    (Or use "⊥" instead of " " so as we keep visibility
    into an internal working of the assimilation engine.)
    """
    result = []
    for s in sausages:
        if len(s) == 2:
            a, b = s
            if len(a)>0 and len(b)>0:
                if a[:-1]==b[:-1] and {a[-1],b[-1]} in [{"_","|"}, {"_"," "}, {" ","|"}]:
                    #s = {a[:-1]+"⊥"}
                    s = {a[:-1]+" "}
        result.append(s)

    return result


def factor_out_common_begins(sausages):
    """
    When alternatives in a sausage happen to start by a common
    substring, factor the substring out to its own one-alternative
    sausage.
    Repeat until length of sausages stops increasing.
    """
    did_change = True # fake it the first time to get in
    while did_change:
        did_change = False
        result = []
        for s in sausages:
            starting_chars = {txt[0] if len(txt)>0 else "" for txt in s}
            if len(starting_chars) == 1:
                result.append(starting_chars) # 1 member set
                rests_of_strings = {txt[1:] if len(txt)>0 else "" for txt in s}
                if rests_of_strings!={""}:
                    did_change = True
                    result.append(rests_of_strings)
            else:
                result.append(s)
        sausages = result
    return sausages

def factor_out_common_ends(sausages):
    """
    When alternatives in a sausage happen to end by a common
    substring, factor the substring out to its own one-alternative
    sausage.
    Repeat until length of sausages stops increasing.
    """
    sausages = reversed_sausages(sausages)
    sausages = factor_out_common_begins(sausages)
    return reversed_sausages(sausages)


def join_simple_sausages(sausages):
    """
    Jouin neighbouring sausages when both have just one alternative
    """
    result = []
    last = ""
    for s in sausages:
        if len(s) == 1:
            txt, = s # get the only element of a set
            last += txt
        else:
            if last != "": # if we accumulated something
                result.append({last})
                last = ""
            result.append(s)
    if last != "":
        result.append({last})
    return result


def to_final_alfabet(txt):
    """
    Convert phonetic symbols to the final alphabet selected
    for presentation.
    """
    if args.ctu_phone_symbols:
        return txt # keep our internal CTU phone symbols as the output format
    return transform(to_cz_transcription, txt)


def sausages_to_printable_string(sausages, use_final_alphabet=True):
    result = ""
    for s in sausages:
        if len(s)==1:
            txt, = s # get the only member of the set
            if use_final_alphabet:
                txt = to_final_alfabet(txt)
            result += txt
        else:
            result += "("
            separator = ""
            for variant in sorted(s):
                if use_final_alphabet:
                    variant = to_final_alfabet(variant)
                result += separator+variant
                separator = ","
            result += ")"
    return result

# Few helper procedures for explicable_sausages() later:

def split_sausages_at_pauses(sausages):  # IS THIS NEEDED ???
    """
    Whenever there is a single alternative containing a simple
    optional pause (" "), split this pause to its own sausage.
    (This implementation in fact splits every character of the
    single alternative to a sausage section of its own - this
    is more than is needed but simpler to implement.)
    """
    result = []
    for s in sausages:
        if len(s)==1:
            txt, = s # get the only member of the set
            acc = ""
            for t in txt:
                if t==" ":
                    if acc!="":
                        result.append({acc})
                        acc = ""
                    result.append({t})
                else:
                    acc += t
            if acc!="":
                result.append({acc})
        else:
            result.append(s)
    return result


def reversed_sausages(sausages):
    """
    Reverse the list of sets, and also each string in each set.
    """
    return [{"".join(reversed(t)) for t in s} for s in reversed(sausages)]


def display_length(text):
    """
    Count how much horicontal space the utf8 string takes in a text terminal,
    supposing correct treatment of the combiing characters we use here.
    """
    zero_len_chars = {"͡", "̥"}
    dl = 0
    for char in text:
        if char not in zero_len_chars:
            dl += 1
    return dl


yellow_background = ("\u001b[48;5;228m", "\u001b[0m") # broken positioning in Mac terminal
darkcyan_foreground = ("\u001b[38;5;31m", "\u001b[0m")
darkblue_foreground = ("\u001b[38;5;12m", "\u001b[0m")


class Screen(list):
    def __init__(self):
        self.base = 0
        self.row_disp_len = {}

    def at(self, row, column, text, color=("", "")):
        """
        Put text at given coordinates on a "screen" which is in fact
        a list of lines. Thus an empty screen is [""] or [] and rows and
        columns are extended as needed to get to the coordinates requested.
        "\n".join(screen) is a printable version.
        This procedure modifies the screen list as needed (and returns nothing).
        Both rows and columns are numbered from 0.
        Every line can only be printed to anywhere after its current end.
        """
        if row<self.base: # if row is more negative than ever seen, shift things
            self[0:0] = [""]*(self.base-row) # insert empty rows at the beginning
            self.base = row # base is some more negative number now
        i_row = row-self.base # i_row is our internal transformed index (never negative)
        rows_now = len(self)
        if rows_now <= i_row: # if row is more positive than ever seen, add rows
            self += [""]*(i_row-rows_now+1)

        if row not in self.row_disp_len: # we keep separate dict with row display lengths
            self.row_disp_len[row] = 0
        row_len = self.row_disp_len[row]
        if row_len < column: # need to add columns to the target line
            self[i_row] += " "*(column-row_len)
            self.row_disp_len[row] += column-row_len
        self[i_row] += color[0]+text+color[1]
        self.row_disp_len[row] += display_length(text)
        # added len without color codes, we know they have zero display length
        # but display_length(text) would not know this (it only knows special phonetic symbols)


def prettyprint_sausage(sausage, phon_from=0):
    """
    Pretty-print sausage with alternatives on multiple lines,
    nicely block aligned. When there are no alternatives, a single
    line of text is produced. When there are occasional alternatives,
    these are printed below to the following line(s).
    Sausages from phon_from are converted to final phonetic alphabet,
    """
    screen = Screen()
    column = 0 # where to print the next sausage section
    direction = 1 # down (1) or up (-1) ?
    snum = 0
    for s in sausage:
        variant_len_max = 0
        row = 0
        for txt in sorted(s):
            if snum>=phon_from:
                txt = to_final_alfabet(txt)
            if txt=="":
                txt = "." # visible sign for 'nothing'
            if len(s)>1:
                screen.at(row, column, txt, darkblue_foreground)
            else:
                screen.at(row, column, txt)
            dl = display_length(txt)
            if dl>variant_len_max:
                variant_len_max = dl
            row += direction
        column += variant_len_max
        if abs(row)>1:
            direction = -direction
        snum += 1
    return "\n".join(screen)





lexicon_replacements = {}

autogen_lexicon_replacements = line_iterable_to_lexirules(autogen_lexicon_replacements_txt.split("\n"))
lexicon_replacements |= autogen_lexicon_replacements

# Automatic rules added first so that even the same lenght match in manual rules
# added below can override them.
lexicon_replacements |= line_iterable_to_lexirules(lexicon_txt.split("\n"))







        
def lookup_string_replacements(txt, string_replacements=lexicon_replacements):
    """
    Lookup the most specific matches in string_replacements
    and return list of possibilities.
    Longest matches are considered first (left to right for
    the same length). When match is found, both the non-matched
    prefix and suffix are recursively processed again (possibly
    finding some shorter match in them again). However, the already
    replaced parts are never searched again for more matches.
    """
    for match_len in range(len(txt),0,-1):
        for pos in range(0, len(txt)-match_len+1):
            x = txt[pos:pos+match_len]
            if x in string_replacements:
                beg = lookup_string_replacements(txt[:pos])
                end = lookup_string_replacements(txt[pos+match_len:])
                mid = string_replacements[x]
                return cartesian_plus(cartesian_plus(beg, mid), end)
    return [txt]


def lookup_word_replacements(word, string_replacements=lexicon_replacements):
    """
    Do lookup_string_replacements() with some additional chores around.
    TODO: Exact match before downcased match?
    """
    word = "+" + word + "+" # mark boundaries
    return [x.strip("+") for x in lookup_string_replacements(word, string_replacements)]
    #NOTE: a bit lazy solution for "+" removal, we might want to limit
    # removel only to "+"s we added above?


def zsplit(txt, splitchars):
    """
    Make couples (separator, chunk)
    """
    retval = []
    separator = ""
    chunk = ""
    for ch in txt:
        if ch in splitchars:
            if chunk!="":
                retval.append((separator, chunk))
                chunk = ""
                separator = ""

            separator += ch
        else:
            chunk += ch
    if chunk!="" or separator!="":
        retval.append((separator, chunk))
    return retval

def text_to_multipron_sausage(txt):
    """
    For every word in txt, consult internal and external
    lexicons of potentially multiple pronunciations.
    Return sausage with word-level alternatives.
    """
    result = []
    for sep, word in zsplit(txt, " _|=~&*"): #NOTE: the only '~'s now are from glue_prepos
        if sep!="":
            result.append({sep})
        if word!="":
            result.append(set(lookup_word_replacements(word)))
    return result


def sausages_transform(table, sausages):
    """
    Apply replacement table on each separate word.
    """
    result = []
    for s in sausages:
        result.append({transform(table, "+"+txt+"+").strip("+") for txt in s})
    return result


def sausages_replacement_rules(repl_rules, sausages):
    """
    Apply replacement rules on each separate word.
    Words are expected to form branches of sausage sections,
    while spaces or other separators should be in their own
    (likely one-alternative) sections.
    Can increase number of word alternatives.
    """
    result = []
    for s in sausages:
        s_out = set()
        for word in s:
            s_out |= set(lookup_word_replacements(word, repl_rules))
        result.append(s_out)
    return result


def sausages_remove_nonphones(sausages, phones="?AEGHNOZabcdefghjklmnoprstuvyz|áéóúýčďňŘřšťŽž", keep_also=""):
    """
    Remove any out-of-phonetic-alphabet phones.
    """
    result = []
    for s in sausages:
        s_out = set()
        for txt in s:
            s_out.add("".join([p for p in txt if p in phones+keep_also]))
        result.append(s_out)
    return result



class MarkableString(str):
    """
    Like normal string but can smuggle additional information
    in atributes (added ad libitum).
    """
    pass

class MarkableList(list):
    """
    Like normal list but can smuggle additional information
    in atributes (added ad libitum).
    """
    pass




def process(txt, all_begins=True, all_ends=True):
    """
    Convert text to pronunciation, using these tables/procedures:
    downcasetab, glue_prepos, multipron (lexicon_replacements)
    phonetizetab, rr_forward_assim
    phone_merging
    all assim
    to final alphabet
    """

    txt = make_text_unix_clean_and_NFC(txt) # ensure LF, NFC, no BOMs. Adds final LF if missing.
    # NOTE: The above operation gets rid of BOM in ANY LINE should there be some.
    # These BOMs could arise e.g. by cut-n-paste of starting lines from multiple text files on Windows.

    txt = txt.rstrip("\r\n") # Final LF would break all_ends IF glue_prepos() is commented out, so kill it here

    words = transform(interpunctab, txt) # clean interpunction (skipped spelltab)

    words_glued = transform(downcasetab, words)
    words_glued = glue_prepos(words_glued)

    words = words.split() # final LF gone here in 'word' branch
    words_glued = words_glued.split()

    txt = transform(spelltab, txt) # B. -> bé  etc.
    txt = transform(interpunctab, txt) # clean interpunction
    txt = transform(downcasetab, txt)

    txt = glue_prepos(txt)  # would also remove final LF in 'phone' branch should there be any (is not)
    # Getting rid of final LF is necessary for all_ends below to work!

    if all_begins:
        txt = "&"+txt # the "&" forces both versions in maybe-glottal-stop situations at start
    else:
        txt = "|"+txt # the "|" is an imaginary space, forcing glottal stops before initial vowels
    if all_ends:
        txt = txt+"*" # the "*" forces generating both voiced and devoiced versions of ends

    ssgs = text_to_multipron_sausage(txt) # apply foreign (and strange) words rules here

    ssgs = sausages_transform(phonetizetab, ssgs) # convert to internal phonetic alphabet
    ssgs = sausages_transform(rr_forward_assim, ssgs) # do that little bit of forward assimilations
    ssgs = sausages_replacement_rules(phone_merging, ssgs) # do it here while we have unbroken words


    #if mark_words:
    #    ...


    ssgs = ssgs_process_phones(ssgs) # do assimilations working backward (and related processing)

    # Phonetic processing is done by now, below we just optimize/prettify the graph
    # which got out of order when composition of multitude of non-deterministic FSTs
    # was applied in ssgs_process_phones and graph was only partially re-optimized.

    ssgs = reintroduce_space(ssgs) # this step might be omitted
    ssgs = factor_out_common_begins(ssgs)
    ssgs = factor_out_common_ends(ssgs)
    ssgs = bme_factorize_sausages(ssgs)
    ssgs = reintroduce_space(ssgs)  # merge any "|" or "_" choice to equivalent " "

    if len(ssgs)>0 and ssgs[0]=={'|'}: # it is that imaginary pause we faked in above
        ssgs = ssgs[1:]                # remove it to make things tidy

    ssgs = join_simple_sausages(ssgs)


    ssgs = MarkableList(ssgs) # add ability to smugle along words
    ssgs.words = words
    ssgs.words_glued = words_glued

    return ssgs


class a:
    pass
args = a() # if we are used as a library, this object can be used to pass us global options


if (__name__ == '__main__'):
    if len(sys.argv)==1: # no argumnents at all
        print(intro)
        sys.exit(0)

    parser = argparse.ArgumentParser(description='Guess Czech pronunciations of text.')

    parser.add_argument('-p', '--print-prons', action='store_true',
                        help='Read text on stdin, output possible pronunciations on color terminal')

    parser.add_argument('-s', '--spartan', action='store_true',
                        help='Use laconic (v1,v2) format to print pronunciations, not multiline one')

    parser.add_argument('--help-symbols', action='store_true',
                        help='Print help for phonetic symbols used in the output')

    parser.add_argument('--all-begins', action='store_true',
                        help='Also create variants which may arise if other speech immediately precedes')

    parser.add_argument('--all-ends', action='store_true',
                        help='Also create variants which may arise if other speech immediately follows')

    parser.add_argument('-c', '--ctu-phone-symbols', action='store_true',
                        help='Use CTU phone symbols to print pronunciations')

    parser.add_argument('-e','--exceptions', help='Text file with additional pronunciation rules') 

    args = parser.parse_args()

    if args.exceptions!=None and not os.path.exists(args.exceptions):
        print(f'Pronunciation exceptions file "{args.exceptions}" does not exist.', file=sys.stderr)
        sys.exit(1)

    if args.help_symbols:
        print('"|" .. pause between words, "_" .. no pause, "." .. empty string (in variants), "=" .. seam')
        print(f"List of phone symbols: {[y for x, y in to_cz_transcription]}")

    if args.print_prons or args.spartan:

        if args.exceptions!=None:
            additional_rules = read_lexirules_table(args.exceptions)
            num_rules_before = len(lexicon_replacements)
            lexicon_replacements |= additional_rules
            num_rules_after = len(lexicon_replacements)
            # print change, so as the user can see number of her own rules, not just examples from exceptions.txt:
            print(f'{num_rules_after-num_rules_before} more rule(s) got from {args.exceptions}')

        print("--- reading stdin, printing possible pronunciations ---")
        for sen in sys.stdin:
            print("")
            sen = sen.strip()
            sg = process(sen, all_begins=args.all_begins, all_ends=args.all_ends)
            if args.spartan:
                print(sausages_to_printable_string(sg))
            else:
                print(prettyprint_sausage(sg))
            print("")

            #print(HMM(sen)) # would need import


        sys.exit(0)



