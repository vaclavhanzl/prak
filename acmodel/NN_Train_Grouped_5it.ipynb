{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab82522",
   "metadata": {},
   "source": [
    "# Train Grouped Phones discriminator - 5 iters before re-align\n",
    "Starting with the latest NN phone alignment, train phone groups discriminator. Groups are merged lijke this:\n",
    "- short/long is ignored for vowels (mostly to help ó)\n",
    "- voiced/voiceless is ignored (this groups rare voiced variants of č and c with the frequent voiceless ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c34a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../prongen/hmm_pron.py --in-jupyter\n",
    "%run ../acmodel/plot.py\n",
    "%matplotlib ipympl\n",
    "###%run ../acmodel/matrix.py\n",
    "%run ../acmodel/praat_ifc.py\n",
    "%run ../acmodel/hmm_acmodel.py\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "%run ../acmodel/nn_acmodel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f3e19",
   "metadata": {},
   "source": [
    "## Get training data\n",
    "We previously aligned Czech CommonVoice train set using an ultra-prinmitive HMM/GMM and then NNs. Let's replace every non-silent phone label by either 'c' or 'v' (so ve have 3 labels: \"cv|\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"mega6_training_0028.tsv\"\n",
    "df = pd.read_csv(infile, sep=\"\\t\", keep_default_na=False)\n",
    "hmms = get_training_hmms(infile, derivatives=3)\n",
    "b_log_corr = b_log_corrections(infile) # get b() corrections based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04035f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mfcc, all_targets, b_set = collect_training_material(hmms)\n",
    "\n",
    "#out_size = len(b_set)\n",
    "in_size = hmms[0].mfcc.size(1)\n",
    "\n",
    "\" \".join(b_set), in_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd1496b",
   "metadata": {},
   "source": [
    "### Map phone labels to group representants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec332a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = 'aá eé yý oó uú pb td ťď kg HhG cZ čŽ sz šž fv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a967c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = {}\n",
    "for phone in b_set:\n",
    "    lab[phone] = phone # default to be overwriten below\n",
    "for grp in groups.split():\n",
    "    for p in grp:\n",
    "        lab[p] = grp[0] # first phone in group represents it\n",
    "#lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68523dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc5052",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_targets = \"\".join(lab[p] for p in all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06704cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_targets[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35971193",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_set = sorted({*x_targets})\n",
    "\" \".join(x_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = len(x_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc58a81",
   "metadata": {},
   "source": [
    "## Setup PyTorch training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size, out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(in_size, out_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b750b9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362cd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db775ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_log_corrections_from_targets(targets):\n",
    "    \"\"\"\n",
    "    Compute log(b()) additive correction needed to suppress very frequent\n",
    "    phones and boost rare ones. Use string of all targets as input.\n",
    "    \"\"\"\n",
    "    c=Counter(targets)\n",
    "    return -torch.tensor([count for phone, count in sorted(i for i in c.items())]).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad23ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_log_corr = b_log_corrections_from_targets(x_targets) # get b() corrections based on frequency of new targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99937ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_log_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hmm in hmms: # update targets also in individual hmms (for the first iteration)\n",
    "    hmm.targets = \"\".join(lab[p] for p in hmm.targets)\n",
    "    hmm.b = \"\".join(lab[p] for p in hmm.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42896ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hmm_nn_log_b(hmm, nn_model, full_b_set, b_log_corr=None):\n",
    "    \"\"\"\n",
    "    For a sentence hmm model with an attached mfcc, compute ln(b()) values\n",
    "    for every sound frame and every model state, using NN phone model.\n",
    "    \"\"\"\n",
    "    logits = nn_model(hmm.mfcc.double().to(device)).detach().to('cpu')\n",
    "    \n",
    "    #print(logits)\n",
    "    \n",
    "    \n",
    "    pred_probab = nn.LogSoftmax(dim=1)(logits)\n",
    "    if b_log_corr!=None:\n",
    "        pred_probab += b_log_corr[None]\n",
    "\n",
    "    # Now select b() columns as needed for this hmm\n",
    "    ph_to_i = {ph:i for i, ph in enumerate(full_b_set)} # map phone to column\n",
    "    \n",
    "    idx = torch.tensor([ph_to_i[ph] for ph in hmm.b])\n",
    "    return(pred_probab[:, idx]) # repeat each b() column as needed\n",
    "\n",
    "\n",
    "\n",
    "def viterbi_log_align_nn(hmm, nn_model, full_b_set, timrev=False, b_log_corr=None):\n",
    "    \"\"\"\n",
    "    Align hmm states with mfcc, working with logprobs\n",
    "    \"\"\"\n",
    "    b = compute_hmm_nn_log_b(hmm, nn_model, full_b_set, b_log_corr)\n",
    "    if timrev:\n",
    "        b = b.flip(0)\n",
    "    A = hmm.A\n",
    "    tmax = hmm.mfcc.size()[0]\n",
    "    len_x = len(A)\n",
    "    x_list = [0]+[float('-inf')]*(len_x-1)\n",
    "    x = torch.tensor(x_list)\n",
    "    alpha = [] #growing list of rows with alpha logprobs\n",
    "    A = torch.tensor(hmm.A)\n",
    "    e_e_f, e_e_t = matrix_extra_edges(A) # prepare efficient representation of A\n",
    "    hmm.optimized_edges = e_e_f, e_e_t  # save it for backward pass - DO THIS ELSEWHERE\n",
    "    for row in range(tmax):\n",
    "        s = x.max() #renormalize\n",
    "        x -= s\n",
    "        alpha.append(x.clone())\n",
    "        next_x(x, (e_e_f, e_e_t))\n",
    "        x += b[row]\n",
    "    return torch.stack(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b232c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = SpeechDataset(all_mfcc, x_targets, x_set) # initial alignment\n",
    "\n",
    "\n",
    "for mega_epoch in range(100):\n",
    "    print(f\"============ Training Group Epoch {mega_epoch} =============\")\n",
    "\n",
    "    all_targets = \"\".join([hmm.targets for hmm in hmms])  # collect alignments\n",
    "    training_data.all_targets = all_targets  # just update the object with new targets\n",
    "\n",
    "    train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True) # new dataloader for this alignment\n",
    "\n",
    "    train_n_epochs(train_dataloader, optimizer, model, criterion, 5)\n",
    "\n",
    "    print('Interupted training for re-alignment...')\n",
    "\n",
    "    model.eval() # switch to evaluation mode\n",
    "\n",
    "\n",
    "    for idx, hmm in enumerate(hmms):\n",
    "        if idx%1000==0:\n",
    "            print(f\"Align {idx}\")\n",
    "    \n",
    "        alp = viterbi_log_align_nn(hmm, model, x_set, b_log_corr=b_log_corr*1.0) # b() corrections according to current frame frequency\n",
    "        hmm.intervals = backward_log_alignment_pass_intervals(hmm, alp) # also modifies alp\n",
    "        hmm.indices = i = alp.max(1).indices\n",
    "        s = \"\".join([hmm.b[ii] for ii in i])\n",
    "        hmm.troubling = troubling_alignmet(s)\n",
    "        hmm.targets = \"\".join([hmm.b[ii] for ii in i])\n",
    "\n",
    "\n",
    "    df['targets'] = [hmm.targets for hmm in hmms]\n",
    "\n",
    "    filename_base = f\"group5i_training_{'%04d' % mega_epoch}\"\n",
    "\n",
    "    torch.save(model.state_dict(), filename_base+\".pth\")\n",
    "    df.to_csv(filename_base+\".tsv\", sep=\"\\t\", index=False)\n",
    "    \n",
    "    b_log_corr = b_log_corrections(filename_base+\".tsv\") # get new b() corrections based on frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "alp = viterbi_log_align_nn(hmm, model, x_set, b_log_corr=b_log_corr*1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_log_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462fb23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7759af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmms[0].mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9680b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
