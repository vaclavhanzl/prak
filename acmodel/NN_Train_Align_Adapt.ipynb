{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab82522",
   "metadata": {},
   "source": [
    "# Train & Align NN AM with Win-to-MFCC and simple speaker adaptation\n",
    "Repeatedly re-align phone labels sequence while training the phones model.\n",
    "To avoid proliferation of the more frequent phones (and mostly the silence), we diminish b() probabilities of frequent phones during re-alignment. We use 3 states pre phone.\n",
    "\n",
    "This version of training adds simple speaker adaptation by making average cepstra over the recording visible as NN inputs (very simple i-vector like approach). We split MFCC to 4 groups according to log-energy (split to above average and below average, then each group is further split into two the same way).\n",
    "\n",
    "(Future versions might compute MFCC averages for key frequent phones.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5b785",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74823cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = 'sv400-500_training_0016.tsv'\n",
    "sideview = 17 # how many additional MFCC frames before and after the focus point are seen\n",
    "mid_size = 500\n",
    "filename_base_base = \"sv500s17_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c34a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmm_pron.py library - generate Czech pron HMM. Included to this notebook.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.prompt_container{width: 11ex !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>div.prompt{min-width: 11ex; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "%run ../prongen/hmm_pron.py --in-jupyter\n",
    "%run ../acmodel/plot.py\n",
    "%matplotlib ipympl\n",
    "%run ../acmodel/matrix.py\n",
    "%run ../acmodel/praat_ifc.py\n",
    "%run ../acmodel/hmm_acmodel.py\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "%run ../acmodel/nn_acmodel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f3e19",
   "metadata": {},
   "source": [
    "## Get training data\n",
    "We previously aligned Czech CommonVoice train set using an ultra-prinmitive HMM/GMM and then NNs. Let's continue on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d2f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#infile = \"mega4_training_0021.tsv\"\n",
    "#infile = 'nn_train.tsv'\n",
    "#infile = 'sv200con_training_0004.tsv'\n",
    "#infile = 'sv200c-300_training_0027.tsv'\n",
    "\n",
    "df = pd.read_csv(infile, sep=\"\\t\", keep_default_na=False)\n",
    "hmms = get_training_hmms(infile, derivatives=0)\n",
    "b_log_corr = b_log_corrections(infile) # get b() corrections based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8e853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade to 3 states per phone (just for duration, b() is still shared)\n",
    "for hmm in hmms:\n",
    "    triple_hmm_states(hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04035f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('? A E G H N O Z a b c d e f g h j k l m n o p r s t u v y z | á é ó ú ý č ď ň Ř ř š ť Ž ž',\n",
       " 45,\n",
       " 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mfcc, all_targets, b_set = collect_training_material(hmms)\n",
    "\n",
    "out_size = len(b_set)\n",
    "in_size = hmms[0].mfcc.size(1)\n",
    "\n",
    "\" \".join(b_set), out_size, in_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b3169",
   "metadata": {},
   "source": [
    "## Add speaker vectors (mean cepstra in 4 energy bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592b9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make s-vectors\n",
    "all_speaker_vectors_refs = []\n",
    "for hmm in hmms:\n",
    "    hmm.speaker_vector = mfcc_make_speaker_vector(hmm.mfcc)\n",
    "    ref = hmm.speaker_vector.to(device)\n",
    "    all_speaker_vectors_refs += [ref]*len(hmm.mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256bf7b",
   "metadata": {},
   "source": [
    "## Changes for the Window-to-MFCC input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "953b4f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_size = hmms[0].mfcc.size(1) * (sideview+1+sideview) + 4*13 # added s-vector\n",
    "in_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "622c6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for alignment decoding, change mfcc in all hmms (for training, we already have a copy)\n",
    "# NOTE: Make speaker vectors BEFORE this!\n",
    "for hmm in hmms:\n",
    "    hmm.mfcc = mfcc_win_view(mfcc_add_sideview(hmm.mfcc, sideview=sideview), sideview=sideview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc58a81",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "797a7ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=507, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=500, out_features=45, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(in_size, out_size, mid_size).to(device) # 50 20 100=svec 50=sv50\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47e9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "training_data = SpeechDataset(all_mfcc, all_targets, b_set, sideview=sideview, speaker_vectors=all_speaker_vectors_refs) # initial alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec4bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Train sv500s17_training, Epoch 0 ========\n",
      "[1, 20000] loss: 1.062\n",
      "[1, 40000] loss: 0.764\n",
      "[1, 60000] loss: 0.680\n",
      "[2, 20000] loss: 0.582\n",
      "[2, 40000] loss: 0.565\n",
      "[2, 60000] loss: 0.549\n",
      "[3, 20000] loss: 0.499\n",
      "[3, 40000] loss: 0.496\n",
      "[3, 60000] loss: 0.491\n",
      "[4, 20000] loss: 0.454\n",
      "[4, 40000] loss: 0.457\n",
      "[4, 60000] loss: 0.456\n",
      "[5, 20000] loss: 0.422\n",
      "[5, 40000] loss: 0.429\n",
      "[5, 60000] loss: 0.431\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train sv500s17_training, Epoch 1 ========\n",
      "[1, 20000] loss: 0.354\n",
      "[1, 40000] loss: 0.361\n",
      "[1, 60000] loss: 0.363\n",
      "[2, 20000] loss: 0.332\n",
      "[2, 40000] loss: 0.342\n",
      "[2, 60000] loss: 0.347\n",
      "[3, 20000] loss: 0.318\n",
      "[3, 40000] loss: 0.329\n",
      "[3, 60000] loss: 0.335\n",
      "[4, 20000] loss: 0.308\n",
      "[4, 40000] loss: 0.320\n",
      "[4, 60000] loss: 0.324\n",
      "[5, 20000] loss: 0.299\n",
      "[5, 40000] loss: 0.313\n",
      "[5, 60000] loss: 0.319\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train sv500s17_training, Epoch 2 ========\n",
      "[1, 20000] loss: 0.291\n",
      "[1, 40000] loss: 0.305\n",
      "[1, 60000] loss: 0.310\n",
      "[2, 20000] loss: 0.281\n",
      "[2, 40000] loss: 0.296\n",
      "[2, 60000] loss: 0.301\n",
      "[3, 20000] loss: 0.275\n"
     ]
    }
   ],
   "source": [
    "for mega_epoch in range(0, 30):\n",
    "    print(f\"======= Train {filename_base_base}, Epoch {mega_epoch} ========\")\n",
    "\n",
    "    all_targets = \"\".join([hmm.targets for hmm in hmms])  # collect alignments\n",
    "    training_data.all_targets = all_targets  # just update the object with new targets\n",
    "\n",
    "    train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True) # new dataloader for this alignment\n",
    "\n",
    "    train_n_epochs(train_dataloader, optimizer, model, criterion, 5)\n",
    "\n",
    "    \n",
    "    filename_base = f\"{filename_base_base}_{'%04d' % mega_epoch}\"\n",
    "    torch.save(model.state_dict(), filename_base+\".pth\")\n",
    "\n",
    "    #break\n",
    "\n",
    "    print('Interrupted training for re-alignment...')\n",
    "\n",
    "    model.eval() # switch to evaluation mode\n",
    "\n",
    "\n",
    "    for idx, hmm in enumerate(hmms):\n",
    "        if idx%1000==0:\n",
    "            print(f\"Align {idx}\")   \n",
    "        alp = align_hmm(hmm, model, b_set, b_log_corr=b_log_corr*1.0, group_tripled=True)\n",
    "\n",
    "\n",
    "    df['targets'] = [hmm.targets for hmm in hmms]\n",
    "\n",
    "    df.to_csv(filename_base+\".tsv\", sep=\"\\t\", index=False)\n",
    "    \n",
    "    b_log_corr = b_log_corrections(filename_base+\".tsv\") # get new b() corrections based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa305ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9fe54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323f21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462fb23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7759af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
