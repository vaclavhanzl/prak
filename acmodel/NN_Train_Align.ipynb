{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab82522",
   "metadata": {},
   "source": [
    "# Train & Align NN AM with Win-to-MFCC and simple speaker adaptation\n",
    "Repeatedly re-align phone labels sequence while training the phones model.\n",
    "To avoid proliferation of the more frequent phones (and mostly the silence), we diminish b() probabilities of frequent phones during re-alignment. We use 3 states pre phone.\n",
    "\n",
    "We use simple speaker adaptation by making average cepstra over the recording visible as NN inputs (very simple i-vector like approach). We split MFCC to 4 groups according to log-energy (split to above average and below average, then each group is further split into two the same way). Future versions might compute MFCC averages for key frequent phones.\n",
    "\n",
    "We iterate in the Baum-Welch stype but no GMMs are used, we start directly with a NN which is quite able to get out of the initial mess very quickly.\n",
    "\n",
    "This notebook expects CommonVoice files converted to wavs and initial_train.tsv (made from train.tsv) looking like this:\n",
    "```\n",
    "wav\tsentence\n",
    "/some/path/common_voice_cs_23896695.wav\tVe srovnání s jinými sýry je téměř bez zápachu.\n",
    "/some/path/common_voice_cs_23896696.wav\tNa stavbě se podíleli příslušníci sedmnácti národů.\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5b785",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74823cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = 'initial_train.tsv'# no phone targets in this tsv yet\n",
    "sideview = 9 # how many additional MFCC frames before and after the focus point are seen\n",
    "mid_size = 100\n",
    "filename_base_base = \"default_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6020ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.path[0] != '..':\n",
    "    sys.path[0:0] = ['..'] # prepend main Prak directory\n",
    "from prongen.hmm_pron import *\n",
    "from acmodel.matrix import * # will be removed, mostly old experiments not needed anymore\n",
    "from acmodel.praat_ifc import *\n",
    "from acmodel.hmm_acmodel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c34a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "print(f\"Using {device} device\") # FIXME: does not really use GPU anymore, some tiny glitch...\n",
    "from acmodel.nn_acmodel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c7fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b_set = b123_set # uncomment to use untied tristate models of phones (except silence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f3e19",
   "metadata": {},
   "source": [
    "## Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3035672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac25fe2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(infile, sep=\"\\t\", keep_default_na=False)\n",
    "hmms = get_training_hmms(infile, derivatives=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cfc8634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wav</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...</td>\n",
       "      <td>Ve srovnání s jinými sýry je téměř bez zápachu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...</td>\n",
       "      <td>Na stavbě se podíleli příslušníci sedmnácti ná...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...</td>\n",
       "      <td>Děkuji vám, vaše sdělení je jasné.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...</td>\n",
       "      <td>Při následné záchranné operaci byl zabit i pil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...</td>\n",
       "      <td>Občasně konzumuje i větší hmyz a výjimečně i r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10751</th>\n",
       "      <td>/data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...</td>\n",
       "      <td>Celkově tyto změny v signalizaci negativně ovl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10752</th>\n",
       "      <td>/data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...</td>\n",
       "      <td>Zvyk je tendence vykonávat za určitých okolnos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10753</th>\n",
       "      <td>/data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...</td>\n",
       "      <td>Jeho žena Marie byla mladší sestra spisovatele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10754</th>\n",
       "      <td>/data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...</td>\n",
       "      <td>Za stejnou roli získal i Oscara.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10755</th>\n",
       "      <td>/data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...</td>\n",
       "      <td>Brzy po narození se rodina přestěhovala do Prahy.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10756 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     wav  \\\n",
       "0      /data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...   \n",
       "1      /data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...   \n",
       "2      /data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...   \n",
       "3      /data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...   \n",
       "4      /data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...   \n",
       "...                                                  ...   \n",
       "10751  /data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...   \n",
       "10752  /data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...   \n",
       "10753  /data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...   \n",
       "10754  /data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...   \n",
       "10755  /data4T/commonvoice/cv-corpus-7.0-2021-07-21/c...   \n",
       "\n",
       "                                                sentence  \n",
       "0        Ve srovnání s jinými sýry je téměř bez zápachu.  \n",
       "1      Na stavbě se podíleli příslušníci sedmnácti ná...  \n",
       "2                     Děkuji vám, vaše sdělení je jasné.  \n",
       "3      Při následné záchranné operaci byl zabit i pil...  \n",
       "4      Občasně konzumuje i větší hmyz a výjimečně i r...  \n",
       "...                                                  ...  \n",
       "10751  Celkově tyto změny v signalizaci negativně ovl...  \n",
       "10752  Zvyk je tendence vykonávat za určitých okolnos...  \n",
       "10753  Jeho žena Marie byla mladší sestra spisovatele...  \n",
       "10754                   Za stejnou roli získal i Oscara.  \n",
       "10755  Brzy po narození se rodina přestěhovala do Prahy.  \n",
       "\n",
       "[10756 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8e853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hmm in hmms:\n",
    "    triple_hmm_states(hmm) # Upgrade to 3 states per phone (just for duration, b() is still shared)\n",
    "    #multiply_hmm_states(hmm)\n",
    "    #triple_hmm_states(hmm, untied=True) # use 3 different states for all nonsilent phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52fb2d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_start_targets(hmm):\n",
    "    \"\"\"\n",
    "    Create mostly fictional targets for direct launch of the NN training.\n",
    "    Except some silence at the begining/end, most targets will be false.\n",
    "    We just take the b string as it is (with triple states), even with\n",
    "    variants (!) and put it in the middle of the training data, with silence\n",
    "    around.\n",
    "    \"\"\"\n",
    "    states = len(hmm.b)\n",
    "    frames = hmm.mfcc.size()[0]\n",
    "    before = (frames-states)//2\n",
    "    after = frames-before-states\n",
    "    hmm.targets = '|'*before+hmm.b+'|'*after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d687df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hmm in hmms:\n",
    "    create_start_targets(hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed7e2a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||vvveee|||sssrrrooovvvnnnáááňňňýýý|||sssjjjyyynnnýýýmmmyyy|||sssýýýrrryyy|||jjjeee|||tttééémmmňňňeeeŘŘŘ|||řřřbbbeeezzzzzzááápppaaaHHHuuu|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmms[0].targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "473b3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['targets'] = [hmm.targets for hmm in hmms]\n",
    "tsv_zero = filename_base_base+\"_0000.tsv\"\n",
    "df.to_csv(tsv_zero, sep=\"\\t\", index=False) # artificial start targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d2f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b_log_corr = b_log_corrections(infile) # get b() corrections based on frequency\n",
    "b_log_corr = b_log_corrections(tsv_zero, b_set=b_set) # get b() corrections based on frequency\n",
    "#b_log_corr = torch.tensor([0]*len(b_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "610de62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04035f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('? A E G H N O Z a b c d e f g h j k l m n o p r s t u v y z | á é ó ú ý č ď ň Ř ř š ť Ž ž',\n",
       " 45,\n",
       " 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mfcc, all_targets = collect_training_material(hmms)\n",
    "\n",
    "out_size = len(b_set)\n",
    "in_size = hmms[0].mfcc.size(1)\n",
    "\n",
    "\" \".join(b_set), out_size, in_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b3169",
   "metadata": {},
   "source": [
    "## Add speaker vectors (mean cepstra in 4 energy bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "592b9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make s-vectors\n",
    "all_speaker_vectors_refs = []\n",
    "for hmm in hmms:\n",
    "    hmm.speaker_vector = mfcc_make_speaker_vector(hmm.mfcc)\n",
    "    ref = hmm.speaker_vector.to(device)\n",
    "    all_speaker_vectors_refs += [ref]*len(hmm.mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256bf7b",
   "metadata": {},
   "source": [
    "## Changes for the Window-to-MFCC input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "953b4f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_size = hmms[0].mfcc.size(1) * (sideview+1+sideview) + 4*13 # added s-vector\n",
    "in_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "622c6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for alignment decoding, change mfcc in all hmms (for training, we already have a copy)\n",
    "# NOTE: Make speaker vectors BEFORE this!\n",
    "for hmm in hmms:\n",
    "    hmm.mfcc = mfcc_win_view(mfcc_add_sideview(hmm.mfcc, sideview=sideview), sideview=sideview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc58a81",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "797a7ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=299, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=45, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(in_size, out_size, mid_size).to(device) # 50 20 100=svec 50=sv50\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a47e9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "training_data = SpeechDataset(all_mfcc, all_targets, b_set, sideview=sideview, speaker_vectors=all_speaker_vectors_refs) # initial alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec4bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Train default_training, Epoch 1 ========\n",
      "[1, 20000] loss: 1.479\n",
      "[1, 40000] loss: 1.463\n",
      "[1, 60000] loss: 1.452\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 2 ========\n",
      "[1, 20000] loss: 1.662\n",
      "[1, 40000] loss: 1.614\n",
      "[1, 60000] loss: 1.596\n",
      "[2, 20000] loss: 1.571\n",
      "[2, 40000] loss: 1.565\n",
      "[2, 60000] loss: 1.561\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 3 ========\n",
      "[1, 20000] loss: 1.154\n",
      "[1, 40000] loss: 1.133\n",
      "[1, 60000] loss: 1.128\n",
      "[2, 20000] loss: 1.114\n",
      "[2, 40000] loss: 1.112\n",
      "[2, 60000] loss: 1.110\n",
      "[3, 20000] loss: 1.100\n",
      "[3, 40000] loss: 1.101\n",
      "[3, 60000] loss: 1.101\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 4 ========\n",
      "[1, 20000] loss: 0.891\n",
      "[1, 40000] loss: 0.884\n",
      "[1, 60000] loss: 0.879\n",
      "[2, 20000] loss: 0.870\n",
      "[2, 40000] loss: 0.873\n",
      "[2, 60000] loss: 0.872\n",
      "[3, 20000] loss: 0.866\n",
      "[3, 40000] loss: 0.869\n",
      "[3, 60000] loss: 0.868\n",
      "[4, 20000] loss: 0.865\n",
      "[4, 40000] loss: 0.865\n",
      "[4, 60000] loss: 0.865\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 5 ========\n",
      "[1, 20000] loss: 0.762\n",
      "[1, 40000] loss: 0.760\n",
      "[1, 60000] loss: 0.757\n",
      "[2, 20000] loss: 0.752\n",
      "[2, 40000] loss: 0.754\n",
      "[2, 60000] loss: 0.753\n",
      "[3, 20000] loss: 0.748\n",
      "[3, 40000] loss: 0.752\n",
      "[3, 60000] loss: 0.752\n",
      "[4, 20000] loss: 0.749\n",
      "[4, 40000] loss: 0.749\n",
      "[4, 60000] loss: 0.751\n",
      "[5, 20000] loss: 0.747\n",
      "[5, 40000] loss: 0.748\n",
      "[5, 60000] loss: 0.749\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 6 ========\n",
      "[1, 20000] loss: 0.694\n",
      "[1, 40000] loss: 0.689\n",
      "[1, 60000] loss: 0.693\n",
      "[2, 20000] loss: 0.688\n",
      "[2, 40000] loss: 0.687\n",
      "[2, 60000] loss: 0.687\n",
      "[3, 20000] loss: 0.686\n",
      "[3, 40000] loss: 0.688\n",
      "[3, 60000] loss: 0.687\n",
      "[4, 20000] loss: 0.683\n",
      "[4, 40000] loss: 0.685\n",
      "[4, 60000] loss: 0.688\n",
      "[5, 20000] loss: 0.683\n",
      "[5, 40000] loss: 0.686\n",
      "[5, 60000] loss: 0.686\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 7 ========\n",
      "[1, 20000] loss: 0.647\n",
      "[1, 40000] loss: 0.646\n",
      "[1, 60000] loss: 0.646\n",
      "[2, 20000] loss: 0.642\n",
      "[2, 40000] loss: 0.642\n",
      "[2, 60000] loss: 0.643\n",
      "[3, 20000] loss: 0.639\n",
      "[3, 40000] loss: 0.642\n",
      "[3, 60000] loss: 0.643\n",
      "[4, 20000] loss: 0.639\n",
      "[4, 40000] loss: 0.641\n",
      "[4, 60000] loss: 0.643\n",
      "[5, 20000] loss: 0.639\n",
      "[5, 40000] loss: 0.642\n",
      "[5, 60000] loss: 0.641\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 8 ========\n",
      "[1, 20000] loss: 0.624\n",
      "[1, 40000] loss: 0.626\n",
      "[1, 60000] loss: 0.625\n",
      "[2, 20000] loss: 0.621\n",
      "[2, 40000] loss: 0.621\n",
      "[2, 60000] loss: 0.623\n",
      "[3, 20000] loss: 0.620\n",
      "[3, 40000] loss: 0.621\n",
      "[3, 60000] loss: 0.623\n",
      "[4, 20000] loss: 0.620\n",
      "[4, 40000] loss: 0.621\n",
      "[4, 60000] loss: 0.621\n",
      "[5, 20000] loss: 0.618\n",
      "[5, 40000] loss: 0.620\n",
      "[5, 60000] loss: 0.622\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 9 ========\n",
      "[1, 20000] loss: 0.612\n",
      "[1, 40000] loss: 0.614\n",
      "[1, 60000] loss: 0.615\n",
      "[2, 20000] loss: 0.610\n",
      "[2, 40000] loss: 0.612\n",
      "[2, 60000] loss: 0.612\n",
      "[3, 20000] loss: 0.609\n",
      "[3, 40000] loss: 0.610\n",
      "[3, 60000] loss: 0.611\n",
      "[4, 20000] loss: 0.609\n",
      "[4, 40000] loss: 0.610\n",
      "[4, 60000] loss: 0.611\n",
      "[5, 20000] loss: 0.609\n",
      "[5, 40000] loss: 0.610\n",
      "[5, 60000] loss: 0.611\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 10 ========\n",
      "[1, 20000] loss: 0.600\n",
      "[1, 40000] loss: 0.600\n",
      "[1, 60000] loss: 0.598\n",
      "[2, 20000] loss: 0.596\n",
      "[2, 40000] loss: 0.598\n",
      "[2, 60000] loss: 0.598\n",
      "[3, 20000] loss: 0.596\n",
      "[3, 40000] loss: 0.597\n",
      "[3, 60000] loss: 0.599\n",
      "[4, 20000] loss: 0.596\n",
      "[4, 40000] loss: 0.597\n",
      "[4, 60000] loss: 0.598\n",
      "[5, 20000] loss: 0.594\n",
      "[5, 40000] loss: 0.598\n",
      "[5, 60000] loss: 0.598\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 11 ========\n",
      "[1, 20000] loss: 0.593\n",
      "[1, 40000] loss: 0.591\n",
      "[1, 60000] loss: 0.592\n",
      "[2, 20000] loss: 0.590\n",
      "[2, 40000] loss: 0.591\n",
      "[2, 60000] loss: 0.592\n",
      "[3, 20000] loss: 0.589\n",
      "[3, 40000] loss: 0.591\n",
      "[3, 60000] loss: 0.591\n",
      "[4, 20000] loss: 0.588\n",
      "[4, 40000] loss: 0.591\n",
      "[4, 60000] loss: 0.590\n",
      "[5, 20000] loss: 0.589\n",
      "[5, 40000] loss: 0.589\n",
      "[5, 60000] loss: 0.591\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 12 ========\n",
      "[1, 20000] loss: 0.579\n",
      "[1, 40000] loss: 0.581\n",
      "[1, 60000] loss: 0.580\n",
      "[2, 20000] loss: 0.577\n",
      "[2, 40000] loss: 0.579\n",
      "[2, 60000] loss: 0.581\n",
      "[3, 20000] loss: 0.576\n",
      "[3, 40000] loss: 0.579\n",
      "[3, 60000] loss: 0.580\n",
      "[4, 20000] loss: 0.575\n",
      "[4, 40000] loss: 0.578\n",
      "[4, 60000] loss: 0.581\n",
      "[5, 20000] loss: 0.576\n",
      "[5, 40000] loss: 0.578\n",
      "[5, 60000] loss: 0.580\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 13 ========\n",
      "[1, 20000] loss: 0.576\n",
      "[1, 40000] loss: 0.579\n",
      "[1, 60000] loss: 0.579\n",
      "[2, 20000] loss: 0.575\n",
      "[2, 40000] loss: 0.577\n",
      "[2, 60000] loss: 0.579\n",
      "[3, 20000] loss: 0.575\n",
      "[3, 40000] loss: 0.577\n",
      "[3, 60000] loss: 0.577\n",
      "[4, 20000] loss: 0.574\n",
      "[4, 40000] loss: 0.578\n",
      "[4, 60000] loss: 0.577\n",
      "[5, 20000] loss: 0.573\n",
      "[5, 40000] loss: 0.577\n",
      "[5, 60000] loss: 0.578\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 14 ========\n",
      "[1, 20000] loss: 0.575\n",
      "[1, 40000] loss: 0.576\n",
      "[1, 60000] loss: 0.577\n",
      "[2, 20000] loss: 0.574\n",
      "[2, 40000] loss: 0.575\n",
      "[2, 60000] loss: 0.575\n",
      "[3, 20000] loss: 0.571\n",
      "[3, 40000] loss: 0.575\n",
      "[3, 60000] loss: 0.575\n",
      "[4, 20000] loss: 0.571\n",
      "[4, 40000] loss: 0.574\n",
      "[4, 60000] loss: 0.577\n",
      "[5, 20000] loss: 0.573\n",
      "[5, 40000] loss: 0.575\n",
      "[5, 60000] loss: 0.575\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 15 ========\n",
      "[1, 20000] loss: 0.569\n",
      "[1, 40000] loss: 0.572\n",
      "[1, 60000] loss: 0.575\n",
      "[2, 20000] loss: 0.570\n",
      "[2, 40000] loss: 0.572\n",
      "[2, 60000] loss: 0.570\n",
      "[3, 20000] loss: 0.568\n",
      "[3, 40000] loss: 0.571\n",
      "[3, 60000] loss: 0.571\n",
      "[4, 20000] loss: 0.567\n",
      "[4, 40000] loss: 0.572\n",
      "[4, 60000] loss: 0.571\n",
      "[5, 20000] loss: 0.568\n",
      "[5, 40000] loss: 0.571\n",
      "[5, 60000] loss: 0.571\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 16 ========\n",
      "[1, 20000] loss: 0.567\n",
      "[1, 40000] loss: 0.568\n",
      "[1, 60000] loss: 0.570\n",
      "[2, 20000] loss: 0.567\n",
      "[2, 40000] loss: 0.569\n",
      "[2, 60000] loss: 0.568\n",
      "[3, 20000] loss: 0.567\n",
      "[3, 40000] loss: 0.568\n",
      "[3, 60000] loss: 0.568\n",
      "[4, 20000] loss: 0.566\n",
      "[4, 40000] loss: 0.567\n",
      "[4, 60000] loss: 0.570\n",
      "[5, 20000] loss: 0.566\n",
      "[5, 40000] loss: 0.565\n",
      "[5, 60000] loss: 0.569\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 17 ========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 20000] loss: 0.563\n",
      "[1, 40000] loss: 0.566\n",
      "[1, 60000] loss: 0.569\n",
      "[2, 20000] loss: 0.563\n",
      "[2, 40000] loss: 0.565\n",
      "[2, 60000] loss: 0.567\n",
      "[3, 20000] loss: 0.563\n",
      "[3, 40000] loss: 0.566\n",
      "[3, 60000] loss: 0.567\n",
      "[4, 20000] loss: 0.563\n",
      "[4, 40000] loss: 0.566\n",
      "[4, 60000] loss: 0.565\n",
      "[5, 20000] loss: 0.563\n",
      "[5, 40000] loss: 0.564\n",
      "[5, 60000] loss: 0.565\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 18 ========\n",
      "[1, 20000] loss: 0.564\n",
      "[1, 40000] loss: 0.565\n",
      "[1, 60000] loss: 0.564\n",
      "[2, 20000] loss: 0.563\n",
      "[2, 40000] loss: 0.564\n",
      "[2, 60000] loss: 0.566\n",
      "[3, 20000] loss: 0.563\n",
      "[3, 40000] loss: 0.564\n",
      "[3, 60000] loss: 0.566\n",
      "[4, 20000] loss: 0.562\n",
      "[4, 40000] loss: 0.565\n",
      "[4, 60000] loss: 0.564\n",
      "[5, 20000] loss: 0.563\n",
      "[5, 40000] loss: 0.565\n",
      "[5, 60000] loss: 0.564\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 19 ========\n",
      "[1, 20000] loss: 0.568\n",
      "[1, 40000] loss: 0.571\n",
      "[1, 60000] loss: 0.572\n",
      "[2, 20000] loss: 0.569\n",
      "[2, 40000] loss: 0.569\n",
      "[2, 60000] loss: 0.573\n",
      "[3, 20000] loss: 0.568\n",
      "[3, 40000] loss: 0.571\n",
      "[3, 60000] loss: 0.570\n",
      "[4, 20000] loss: 0.568\n",
      "[4, 40000] loss: 0.570\n",
      "[4, 60000] loss: 0.569\n",
      "[5, 20000] loss: 0.568\n",
      "[5, 40000] loss: 0.570\n",
      "[5, 60000] loss: 0.571\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 20 ========\n",
      "[1, 20000] loss: 0.568\n",
      "[1, 40000] loss: 0.569\n",
      "[1, 60000] loss: 0.570\n",
      "[2, 20000] loss: 0.569\n",
      "[2, 40000] loss: 0.568\n",
      "[2, 60000] loss: 0.569\n",
      "[3, 20000] loss: 0.566\n",
      "[3, 40000] loss: 0.569\n",
      "[3, 60000] loss: 0.570\n",
      "[4, 20000] loss: 0.566\n",
      "[4, 40000] loss: 0.567\n",
      "[4, 60000] loss: 0.569\n",
      "[5, 20000] loss: 0.566\n",
      "[5, 40000] loss: 0.568\n",
      "[5, 60000] loss: 0.569\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 21 ========\n",
      "[1, 20000] loss: 0.568\n",
      "[1, 40000] loss: 0.571\n",
      "[1, 60000] loss: 0.569\n",
      "[2, 20000] loss: 0.567\n",
      "[2, 40000] loss: 0.569\n",
      "[2, 60000] loss: 0.571\n",
      "[3, 20000] loss: 0.568\n",
      "[3, 40000] loss: 0.568\n",
      "[3, 60000] loss: 0.571\n",
      "[4, 20000] loss: 0.567\n",
      "[4, 40000] loss: 0.569\n",
      "[4, 60000] loss: 0.569\n",
      "[5, 20000] loss: 0.566\n",
      "[5, 40000] loss: 0.570\n",
      "[5, 60000] loss: 0.570\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 22 ========\n",
      "[1, 20000] loss: 0.562\n",
      "[1, 40000] loss: 0.565\n",
      "[1, 60000] loss: 0.565\n",
      "[2, 20000] loss: 0.563\n",
      "[2, 40000] loss: 0.562\n",
      "[2, 60000] loss: 0.563\n",
      "[3, 20000] loss: 0.561\n",
      "[3, 40000] loss: 0.564\n",
      "[3, 60000] loss: 0.565\n",
      "[4, 20000] loss: 0.561\n",
      "[4, 40000] loss: 0.563\n",
      "[4, 60000] loss: 0.564\n",
      "[5, 20000] loss: 0.562\n",
      "[5, 40000] loss: 0.563\n",
      "[5, 60000] loss: 0.565\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 23 ========\n",
      "[1, 20000] loss: 0.568\n",
      "[1, 40000] loss: 0.569\n",
      "[1, 60000] loss: 0.570\n",
      "[2, 20000] loss: 0.566\n",
      "[2, 40000] loss: 0.570\n",
      "[2, 60000] loss: 0.568\n",
      "[3, 20000] loss: 0.567\n",
      "[3, 40000] loss: 0.568\n",
      "[3, 60000] loss: 0.569\n",
      "[4, 20000] loss: 0.566\n",
      "[4, 40000] loss: 0.568\n",
      "[4, 60000] loss: 0.568\n",
      "[5, 20000] loss: 0.567\n",
      "[5, 40000] loss: 0.568\n",
      "[5, 60000] loss: 0.569\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 24 ========\n",
      "[1, 20000] loss: 0.568\n",
      "[1, 40000] loss: 0.571\n",
      "[1, 60000] loss: 0.569\n",
      "[2, 20000] loss: 0.568\n",
      "[2, 40000] loss: 0.567\n",
      "[2, 60000] loss: 0.571\n",
      "[3, 20000] loss: 0.567\n",
      "[3, 40000] loss: 0.567\n",
      "[3, 60000] loss: 0.570\n",
      "[4, 20000] loss: 0.567\n",
      "[4, 40000] loss: 0.568\n",
      "[4, 60000] loss: 0.570\n",
      "[5, 20000] loss: 0.567\n",
      "[5, 40000] loss: 0.568\n",
      "[5, 60000] loss: 0.569\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 25 ========\n",
      "[1, 20000] loss: 0.567\n",
      "[1, 40000] loss: 0.570\n",
      "[1, 60000] loss: 0.570\n",
      "[2, 20000] loss: 0.566\n",
      "[2, 40000] loss: 0.568\n",
      "[2, 60000] loss: 0.571\n",
      "[3, 20000] loss: 0.567\n",
      "[3, 40000] loss: 0.568\n",
      "[3, 60000] loss: 0.568\n",
      "[4, 20000] loss: 0.567\n",
      "[4, 40000] loss: 0.570\n",
      "[4, 60000] loss: 0.568\n",
      "[5, 20000] loss: 0.566\n",
      "[5, 40000] loss: 0.569\n",
      "[5, 60000] loss: 0.570\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 26 ========\n",
      "[1, 20000] loss: 0.567\n",
      "[1, 40000] loss: 0.568\n",
      "[1, 60000] loss: 0.570\n",
      "[2, 20000] loss: 0.566\n",
      "[2, 40000] loss: 0.568\n",
      "[2, 60000] loss: 0.569\n",
      "[3, 20000] loss: 0.567\n",
      "[3, 40000] loss: 0.568\n",
      "[3, 60000] loss: 0.569\n",
      "[4, 20000] loss: 0.566\n",
      "[4, 40000] loss: 0.568\n",
      "[4, 60000] loss: 0.569\n",
      "[5, 20000] loss: 0.565\n",
      "[5, 40000] loss: 0.567\n",
      "[5, 60000] loss: 0.569\n",
      "Interrupted training for re-alignment...\n",
      "Align 0\n",
      "Align 1000\n",
      "Align 2000\n",
      "Align 3000\n",
      "Align 4000\n",
      "Align 5000\n",
      "Align 6000\n",
      "Align 7000\n",
      "Align 8000\n",
      "Align 9000\n",
      "Align 10000\n",
      "======= Train default_training, Epoch 27 ========\n",
      "[1, 20000] loss: 0.570\n"
     ]
    }
   ],
   "source": [
    "for mega_epoch in range(1, 50): # starting from 1 as we have zero tsv\n",
    "    print(f\"======= Train {filename_base_base}, Epoch {mega_epoch} ========\")\n",
    "\n",
    "    all_targets = \"\".join([hmm.targets for hmm in hmms])  # collect alignments\n",
    "    training_data.all_targets = all_targets  # just update the object with new targets\n",
    "\n",
    "    train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True) # new dataloader for this alignment\n",
    "\n",
    "    train_n_epochs(train_dataloader, optimizer, model, criterion, 5 if mega_epoch>=5 else mega_epoch) # at start, align more often\n",
    "\n",
    "    \n",
    "    filename_base = f\"{filename_base_base}_{'%04d' % mega_epoch}\"\n",
    "    torch.save(model.state_dict(), filename_base+\".pth\")\n",
    "\n",
    "    #break\n",
    "\n",
    "    print('Interrupted training for re-alignment...')\n",
    "\n",
    "    model.eval() # switch to evaluation mode\n",
    "\n",
    "\n",
    "    for idx, hmm in enumerate(hmms):\n",
    "        if idx%1000==0:\n",
    "            print(f\"Align {idx}\")   \n",
    "        alp = align_hmm(hmm, model, b_set, b_log_corr=b_log_corr, group_tripled=True)\n",
    "        #alp = align_hmm(hmm, model, b_set, b_log_corr=b_log_corr*1.0, group_tripled=False)\n",
    "\n",
    "\n",
    "    df['targets'] = [hmm.targets for hmm in hmms]\n",
    "\n",
    "    df.to_csv(filename_base+\".tsv\", sep=\"\\t\", index=False)\n",
    "    \n",
    "    b_log_corr = b_log_corrections(filename_base+\".tsv\", b_set=b_set) # get new b() corrections based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7759af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
