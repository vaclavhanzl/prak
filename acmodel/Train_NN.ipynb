{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab82522",
   "metadata": {},
   "source": [
    "# Train NN Acoustic Model\n",
    "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c34a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../prongen/hmm_pron.py --in-jupyter\n",
    "%run ../acmodel/plot.py\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f3e19",
   "metadata": {},
   "source": [
    "## Get training data\n",
    "We aligned Czech CommonVoice train set using an ultra-prinmitive HMM/GMM. Let's use it as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bacc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#f = pd.read_csv(\"nn_train.tsv\", sep=\"\\t\", keep_default_na=False)\n",
    "#df = pd.read_csv(\"nn_train_g2.tsv\", sep=\"\\t\", keep_default_na=False)\n",
    "#df = pd.read_csv(\"nn_train_g3.tsv\", sep=\"\\t\", keep_default_na=False)\n",
    "df = pd.read_csv(\"nn_train_g4.tsv\", sep=\"\\t\", keep_default_na=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmms = []\n",
    "for wav, sentence, targets in list(zip(df.wav.values, df.sentence.values, df.targets.values)):\n",
    "    hmm = HMM(sentence, wav=wav, use_DA=True)\n",
    "    hmm.targets = targets\n",
    "    hmms.append(hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04035f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_set = sorted({*\"\".join([hmm.b for hmm in hmms ])}) # make sorted set of all phone names in the training set\n",
    "out_size = len(b_set)\n",
    "in_size = hmms[0].mfcc.size(1)\n",
    "\" \".join(b_set), out_size, in_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = \"\".join([hmm.targets for hmm in hmms])\n",
    "train_len = len(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mfcc = torch.cat([hmm.mfcc for hmm in hmms]).double().to(device)\n",
    "#all_mfcc.to(device)\n",
    "assert all_mfcc.size()[0]==train_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc58a81",
   "metadata": {},
   "source": [
    "## Setup PyTorch training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b75ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "#from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4da8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, all_mfcc, all_targets, b_set):\n",
    "        self.all_mfcc = all_mfcc\n",
    "        self.all_targets = all_targets\n",
    "        \n",
    "        self.wanted_outputs = torch.eye(len(b_set), device=device).double()\n",
    "        self.output_map = {}\n",
    "        for i, b in enumerate(b_set):\n",
    "            self.output_map[b] = self.wanted_outputs[i] # prepare outputs with one 1 at the right place\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.all_mfcc[idx], self.output_map[self.all_targets[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = SpeechDataset(all_mfcc, all_targets, b_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ff30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1def29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_size)\n",
    "            #nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3bc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fa8675",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b0fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20000 == 19999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[19, 20000] loss: 1.181 1.187 1.190 1.179 1.184 [20, 60000] loss: 1.187\n",
    "#\n",
    "# g2:[19, 20000] loss: 0.997 1.003 1.008 0.995 1.002 [20, 60000] loss: 1.002\n",
    "# g3: [19, 20000] loss: 0.950 0.953 0.955 0.947 0.950 [20, 60000] loss: 0.954\n",
    "# g4: [19, 20000] loss: 0.922 0.927 0.928 0.920 0.924 [20, 60000] loss: 0.925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c14f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights_g4_20.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30201b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20000 == 19999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[19, 20000] loss: 1.149 1.153 1.155 1.146 1.152 #[20, 60000] loss 1.156\n",
    "#\n",
    "# g2:[19, 20000] loss: 0.961 0.969 0.973 0.961 0.968 [20, 60000] loss: 0.971\n",
    "# g3: [19, 20000] loss: 0.911 0.918 0.922 0.910 0.918 [20, 60000] loss: 0.919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'model_weights_g2_40.pth')\n",
    "torch.save(model.state_dict(), 'model_weights_g4_40.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc371c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5b4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c67a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff77654",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3cfd55",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea602a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20000 == 19999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58337a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights_60.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(40):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20000 == 19999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb908a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights_100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bc4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(40):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20000 == 19999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights_140.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7759af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
